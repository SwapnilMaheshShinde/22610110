{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#1ST PRAC basic operations with TensorFlow 2 tensors Eager Execution\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "a = tf.constant([[1, 2], [3, 4]])\n",
        "b = tf.constant([[5, 6], [7, 8]])\n",
        "\n",
        "print(\"Add:\\n\", a + b)\n",
        "print(\"MatMul:\\n\", a @ b)\n",
        "print(\"ElemMul:\\n\", a * b)\n"
      ],
      "metadata": {
        "id": "Sf5p_H3VipuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2ND PRAC Preprocess and clean datasets for Generative AI\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'Name': ['A', 'B', 'C', None],\n",
        "        'Age': [25, np.nan, 22, 28],\n",
        "        'City': ['Pune', 'Mumbai', 'Delhi', 'Pune']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handle missing data (updated method)\n",
        "df['Name'] = df['Name'].fillna('Unknown')\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
        "\n",
        "# Normalize numerical column\n",
        "df['Age'] = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\n",
        "\n",
        "# Encode categorical variable\n",
        "df = pd.get_dummies(df, columns=['City'])\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "JIlRjPkwiy6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3RD PRAC Use Matplotlib or Seaborn to visualize data\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "# 1️⃣ Histogram\n",
        "sns.histplot(df['sepal_length'], kde=True)\n",
        "plt.title(\"Sepal Length Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# 2️⃣ Scatter plot\n",
        "sns.scatterplot(x='sepal_length', y='petal_length', hue='species', data=df)\n",
        "plt.title(\"Sepal vs Petal Length\")\n",
        "plt.show()\n",
        "\n",
        "# 3️⃣ Correlation heatmap (numeric columns only)\n",
        "numeric_df = df.select_dtypes(include=['float', 'int'])  # Filter numeric columns\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y4UgjtNbi2Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRAC 4 Implement word embeddings (e.g., Word2Vec or GloVe)\n",
        "!pip install faiss-cpu\n",
        "import gensim.downloader as api, faiss, numpy as np\n",
        "\n",
        "m = api.load('glove-wiki-gigaword-50')\n",
        "vecs = np.array([m[w] for w in m.index_to_key[:5000]], 'float32')\n",
        "idx = faiss.IndexFlatL2(50); idx.add(vecs)\n",
        "\n",
        "word = \"car\"\n",
        "D, I = idx.search(np.array([m[word]], 'float32'), 5)\n",
        "print(f\"Words semantically similar to '{word}':\", [m.index_to_key[i] for i in I[0]])\n"
      ],
      "metadata": {
        "id": "gLoniRMai4eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRAC 5 pre-trained diffusion model (e.g., Denoising Diffusion Probabilistic\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess MNIST\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\")\n",
        "\n",
        "# Create a simple random generator (for demo only)\n",
        "noise = tf.random.normal((16, 28, 28, 1))\n",
        "generated = noise  # since we don’t have pretrained model\n",
        "\n",
        "# Display random \"generated\" images\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(generated[i,:,:,0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K1JqSCkLn2QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRAC 6 CLIP-guided diffusion model to perform text-to-image generation.\n",
        "\n",
        "\n",
        "# Simple Text-to-Image using Pollinations API + CLIP similarity\n",
        "# Fast, lightweight, and shows image inline\n",
        "\n",
        "!pip install -q git+https://github.com/openai/CLIP.git torchvision pillow requests\n",
        "\n",
        "import torch, requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import clip\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Text prompt\n",
        "prompt = \"A futuristic cityscape with flying cars\"\n",
        "\n",
        "# Generate image quickly using Pollinations API\n",
        "url = f\"https://image.pollinations.ai/prompt/{requests.utils.quote(prompt)}\"\n",
        "image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n",
        "\n",
        "# Show image inline\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(prompt)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate image-text similarity using CLIP\n",
        "text = clip.tokenize([prompt]).to(device)\n",
        "img = preprocess(image).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    score = (model.encode_image(img) @ model.encode_text(text).T).item()\n",
        "\n",
        "print(f\"CLIP Similarity Score: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZowyMgVVjCSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRAC 7 (e.g., DistilGPT-2) using LoRA on a text corpus\n",
        "\n",
        "# Single-cell: LoRA fine-tune (DistilGPT-2) -> generate -> perplexity\n",
        "# If libs missing, it will install a compatible set (may take a minute once).\n",
        "try:\n",
        "    import torch, transformers, peft\n",
        "except Exception:\n",
        "    !pip install -q torch transformers==4.37.2 peft==0.7.1 datasets accelerate==0.25.0\n",
        "    import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "# ----- tiny dataset (for quick demo) -----\n",
        "texts = [\n",
        "  \"AI is transforming technology.\",\n",
        "  \"LoRA fine-tuning saves memory.\",\n",
        "  \"Transformers are great for text generation.\",\n",
        "  \"AI will shape the future.\"\n",
        "] * 40   # repeat to give the model some signal\n",
        "\n",
        "# ----- tokenizer / model / LoRA -----\n",
        "tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tok.pad_token = tok.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "lora_cfg = LoraConfig(r=8, lora_alpha=16, target_modules=[\"c_attn\"], task_type=\"CAUSAL_LM\")\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# ----- tiny Dataset -----\n",
        "class T(Dataset):\n",
        "    def __init__(self, texts, tok):\n",
        "        enc = tok(texts, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "        self.ids, self.att = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
        "    def __len__(self): return self.ids.size(0)\n",
        "    def __getitem__(self, i): return {\"input_ids\": self.ids[i], \"attention_mask\": self.att[i]}\n",
        "\n",
        "ds = T(texts, tok)\n",
        "loader = DataLoader(ds, batch_size=4, shuffle=True)\n",
        "\n",
        "# ----- quick training loop (1 epoch, very short) -----\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "for epoch in range(1):\n",
        "    for batch in loader:\n",
        "        ids = batch[\"input_ids\"].to(device)\n",
        "        att = batch[\"attention_mask\"].to(device)\n",
        "        out = model(input_ids=ids, attention_mask=att, labels=ids)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "        opt.step(); opt.zero_grad()\n",
        "    print(f\"Epoch {epoch+1} loss: {loss.item():.4f}\")\n",
        "\n",
        "# ----- generate a sample -----\n",
        "model.eval()\n",
        "prompt = \"Artificial intelligence will\"\n",
        "ids = tok(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "gen = model.generate(ids, max_length=50, do_sample=True, top_p=0.9, num_return_sequences=1)\n",
        "print(\"\\n=== GENERATED ===\\n\", tok.decode(gen[0], skip_special_tokens=True))\n",
        "\n",
        "# ----- approximate perplexity on few training samples -----\n",
        "model.eval()\n",
        "losses = []\n",
        "with torch.no_grad():\n",
        "    for i in range(8):\n",
        "        item = ds[i]\n",
        "        ids = item[\"input_ids\"].unsqueeze(0).to(device)\n",
        "        out = model(input_ids=ids, labels=ids)\n",
        "        losses.append(out.loss.item())\n",
        "ppl = math.exp(sum(losses)/len(losses))\n",
        "print(f\"\\nApprox Perplexity: {ppl:.2f}\")\n"
      ],
      "metadata": {
        "id": "P4h3Hb13s-HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRAC 8 Evaluate outputs using BLEU or ROUGE metrics.\n",
        "!pip install evaluate\n",
        "from transformers import pipeline\n",
        "import evaluate\n",
        "!pip install rouge_score\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "text = \"Artificial Intelligence enables machines to learn from data and perform tasks that normally need human intelligence.\"\n",
        "summary = summarizer(text, max_length=40, min_length=10, do_sample=False)[0]['summary_text']\n",
        "\n",
        "qa = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "ans = qa(question=\"Who developed GPT models?\", context=\"GPT models are large language models developed by OpenAI.\")['answer']\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "score = rouge.compute(predictions=[summary], references=[\"AI helps machines perform human-like tasks.\"])\n",
        "\n",
        "print(\"Summary:\", summary)\n",
        "print(\"Answer:\", ans)\n",
        "print(\"ROUGE Score:\", score[\"rouge1\"])\n"
      ],
      "metadata": {
        "id": "PVKaq2pzlyJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRAC 9: Retrieval-Augmented Generation (RAG)\n",
        "!pip install -q faiss-cpu sentence-transformers transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss, numpy as np\n",
        "\n",
        "# 1) Small document collection\n",
        "docs = [\n",
        "  \"Paris is the capital of France and is known for the Eiffel Tower.\",\n",
        "  \"The Nile is the longest river in Africa and flows through Egypt.\",\n",
        "  \"Python is a popular programming language for data science and AI.\",\n",
        "  \"The mitochondrion is the powerhouse of the cell.\",\n",
        "  \"Cricket is a bat-and-ball game popular in India, England, and Australia.\"\n",
        "]\n",
        "\n",
        "# 2) Create embeddings\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "doc_embs = embedder.encode(docs, convert_to_numpy=True).astype(\"float32\")\n",
        "\n",
        "# 3) Build FAISS index\n",
        "index = faiss.IndexFlatL2(doc_embs.shape[1])\n",
        "index.add(doc_embs)\n",
        "\n",
        "# 4) Ask question\n",
        "question = \"Which city has the Eiffel Tower?\"\n",
        "\n",
        "# 5) Retrieve top-2 relevant documents\n",
        "q_emb = embedder.encode([question], convert_to_numpy=True).astype(\"float32\")\n",
        "D, I = index.search(q_emb, k=2)\n",
        "retrieved_docs = [docs[i] for i in I[0]]\n",
        "\n",
        "# 6) Combine retrieved context\n",
        "context = \" \".join(retrieved_docs)\n",
        "\n",
        "# 7) Use QA model for final answer\n",
        "qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "answer = qa(question=question, context=context)\n",
        "\n",
        "print(\"Retrieved Documents:\")\n",
        "for r in retrieved_docs:\n",
        "    print(\"-\", r)\n",
        "\n",
        "print(\"\\nQuestion:\", question)\n",
        "print(\"Answer:\", answer['answer'])\n"
      ],
      "metadata": {
        "id": "JJJwk7Z0moAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRAC 10 Apply model optimization techniques such as pruning or quantization\n",
        "\n",
        "# Quantize DistilBERT and test inference speed\n",
        "!pip install -q transformers torch\n",
        "\n",
        "import torch, time\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# Load model and tokenizer\n",
        "tok = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Sample input\n",
        "text = \"Artificial intelligence is transforming the world.\"\n",
        "inputs = tok(text, return_tensors=\"pt\")\n",
        "\n",
        "# Original model speed\n",
        "start = time.time()\n",
        "_ = model(**inputs)\n",
        "orig_time = time.time() - start\n",
        "\n",
        "# Quantize model (8-bit)\n",
        "quant_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Quantized model speed\n",
        "start = time.time()\n",
        "_ = quant_model(**inputs)\n",
        "quant_time = time.time() - start\n",
        "\n",
        "# Compare\n",
        "print(f\"Original time: {orig_time:.4f}s | Quantized time: {quant_time:.4f}s\")\n",
        "print(f\"Memory reduced from {model.num_parameters()/1e6:.1f}M → {quant_model.num_parameters()/1e6:.1f}M parameters\")\n"
      ],
      "metadata": {
        "id": "ORcXidpznC-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRAC 11: Bias in the outputs of a pre-trained generative model\n",
        "!pip install -q transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-2 model\n",
        "gen = pipeline(\"text-generation\", model=\"gpt2\", pad_token_id=50256)\n",
        "\n",
        "# Prompts that might show bias\n",
        "prompts = [\"The nurse said that\", \"The engineer said that\"]\n",
        "\n",
        "print(\"=== Raw Model Outputs (Potential Bias) ===\")\n",
        "for p in prompts:\n",
        "    out = gen(\n",
        "        p,\n",
        "        max_length=25,          # slightly longer for smoother completion\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,              # nucleus sampling to reduce repetition\n",
        "        temperature=0.7,         # more controlled generation\n",
        "    )[0]['generated_text']\n",
        "    print(f\"\\nPrompt: {p}\\nOutput: {out.strip()}\")\n",
        "\n",
        "# --- Simple bias mitigation: remove gendered words ---\n",
        "def mitigate_bias(text):\n",
        "    gendered_words = {\"he\", \"she\", \"him\", \"her\", \"his\", \"hers\"}\n",
        "    clean = \" \".join([w for w in text.split() if w.lower() not in gendered_words])\n",
        "    return clean\n",
        "\n",
        "print(\"\\n=== After Simple Bias Mitigation ===\")\n",
        "for p in prompts:\n",
        "    out = gen(\n",
        "        p,\n",
        "        max_length=25,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "    )[0]['generated_text']\n",
        "    mitigated = mitigate_bias(out.strip())\n",
        "    print(f\"\\nPrompt: {p}\\nOutput: {mitigated}\")\n"
      ],
      "metadata": {
        "id": "iC2ofHN-nswl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRAC 12 simple autonomous AI agent using Python for a text-based task\n",
        "\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "agent = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "def ai_agent(name, message):\n",
        "    response = agent(message, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
        "    print(f\"\\n{name}: {response}\\n\")\n",
        "    return response\n",
        "\n",
        "msg = \"Hello! What do you think about AI?\"\n",
        "for i in range(3):\n",
        "    msg = ai_agent(\"Agent 1\", msg)\n",
        "    time.sleep(1)\n",
        "    msg = ai_agent(\"Agent 2\", msg)\n",
        "    time.sleep(1)\n"
      ],
      "metadata": {
        "id": "FF8kZVvvpxqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "agent = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "def ai_agent(name, message):\n",
        "    response = agent(message, max_length=40, truncation=True)[0]['generated_text']\n",
        "    print(f\"{name}: {response}\\n\")\n",
        "    return response\n",
        "\n",
        "msg = \"Hello! What do you think about AI?\"\n",
        "for i in range(1):\n",
        "    msg = ai_agent(\"Agent 1\", msg)\n",
        "    msg = ai_agent(\"Agent 2\",msg)"
      ],
      "metadata": {
        "id": "9Crzt-BDrS3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GwuUHdGwLdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}